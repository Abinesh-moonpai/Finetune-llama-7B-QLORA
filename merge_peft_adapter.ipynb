{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from dataclasses import dataclass, field\n", "from typing import Optional"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import peft\n", "import torch\n", "from peft import PeftConfig, PeftModel\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dataclass\n", "class ScriptArguments:\n", "    \"\"\"\n", "    The name of the Casual LM model we wish to fine with PPO\n", "    \"\"\"\n\n", "    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n", "    # models like gpt-neo* models are more suitable\n", "    model_name: Optional[str] = field(default=\"edbeeching/gpt-neo-125M-imdb-lora\", metadata={\"help\": \"the model name\"})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser = HfArgumentParser(ScriptArguments)\n", "script_args = parser.parse_args_into_dataclasses()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["peft_model_id = script_args.model_name\n", "peft_config = PeftConfig.from_pretrained(peft_model_id)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    peft_config.base_model_name_or_path,\n", "    return_dict=True,\n", "    torch_dtype=torch.float16,\n", ")\n", "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the Lora model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = PeftModel.from_pretrained(model, peft_model_id)\n", "model.eval()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n", "for key in key_list:\n", "    parent, target, target_name = model.base_model._get_submodules(key)\n", "    if isinstance(target, peft.tuners.lora.Linear):\n", "        bias = target.bias is not None\n", "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n", "        model.base_model._replace_module(parent, target_name, new_module, target)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = model.base_model.model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.push_to_hub(f\"{script_args.model_name}-adapter-merged\", use_temp_dir=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}