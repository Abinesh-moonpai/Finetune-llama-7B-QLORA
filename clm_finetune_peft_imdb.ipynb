{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dataclasses import dataclass, field\n","from itertools import chain\n","from typing import Optional\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_int8_training\n","from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n","    \"\"\"\n","    model_name_or_path: Optional[str] = field(\n","        default=\"facebook/opt-125m\",\n","        metadata={\n","            \"help\": (\n","                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n","            )\n","        },\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@dataclass\n","class DataTrainingArguments:\n","    dataset_name: Optional[str] = field(\n","        default=\"imdb\", metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )\n","    block_size: Optional[int] = field(\n","        default=1024, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_args, data_args, training_args = parser.parse_args_into_dataclasses()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_args.model_name_or_path,\n","    load_in_8bit=True,\n","    device_map=\"auto\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","# ### Prepare model for training\n","#\n","# Some pre-processing needs to be done before training such an int8 model using `peft`, therefore let's import an utiliy function `prepare_model_for_int8_training` that will:\n","# - Cast the layer norm in `float32` for stability purposes\n","# - Add a `forward_hook` to the input embedding layer to enable gradient computation of the input hidden states\n","# - Enable gradient checkpointing for more memory-efficient training\n","# - Cast the output logits in `float32` for smoother sampling during the sampling procedure"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if \"gpt-neox\" in model_args.model_name_or_path:\n","    model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"embed_out\")\n","else:\n","    model = prepare_model_for_int8_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["### Apply LoRA<br>\n","<br>\n","Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_modules = None\n","if \"gpt-neox\" in model_args.model_name_or_path:\n","    target_modules = [\"query_key_value\", \"xxx\"]  # workaround to use 8bit training on this model\n","config = LoraConfig(\n","    r=16, lora_alpha=32, target_modules=target_modules, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["block_size = data_args.block_size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","    # customize this part to your needs.\n","    if total_length >= block_size:\n","        total_length = (total_length // block_size) * block_size\n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = load_dataset(\"imdb\")\n","columns = data[\"train\"].features\n","data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True, remove_columns=columns)\n","data = data.map(group_texts, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.gradient_checkpointing_enable()\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data[\"train\"],\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Share adapters on the ðŸ¤— Hub"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.push_to_hub(training_args.output_dir, use_auth_token=True)"]},{"cell_type":"markdown","metadata":{},"source":["Load adapters from the Hub and generate some output texts:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["peft_model_id = training_args.output_dir\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"]},{"cell_type":"markdown","metadata":{},"source":["Load the Lora model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = PeftModel.from_pretrained(model, peft_model_id)\n","# You can then directly use the trained model or the model that you have loaded from the ðŸ¤— Hub for inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch = tokenizer(\"I really enjoyed the \", return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.cuda.amp.autocast():\n","    output_tokens = model.generate(**batch, max_new_tokens=50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\\n\\n\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
