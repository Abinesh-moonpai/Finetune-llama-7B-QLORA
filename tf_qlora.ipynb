{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras-nlp -q matplotlib tensorflow_datasets ipywidgets jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = 500\n",
    "EPOCHS = 1  # Can be set to a higher value for better results\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "MAX_GENERATION_LENGTH = 200\n",
    "\n",
    "GPT2_PRESET = \"gpt2_base_en\"\n",
    "\n",
    "# LoRA-specific hyperparameters\n",
    "RANK = 4\n",
    "ALPHA = 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"me and a friend decided to go to the beach last sunday. we loaded up and headed out. we were about half way there when i decided that i was not leaving till i had seafood. \\n\\nnow i'm not talking about red lobster. no friends i'm talking about a low country boil. i found the restaurant and got directions. i don't know if any of you have heard about the crab shack on tybee island but let me tell you it's worth it. \\n\\nwe arrived and was seated quickly. we decided to get a seafood sampler for two and split it. the waitress bought it out on separate platters for us. the amount of food was staggering. two types of crab, shrimp, mussels, crawfish, andouille sausage, red potatoes, and corn on the cob. i managed to finish it and some of my friends crawfish and mussels. it was a day to be a fat ass. we finished paid for our food and headed to the beach. \\n\\nfunny thing about seafood. it runs through me faster than a kenyan \\n\\nwe arrived and walked around a bit. it was about 45min since we arrived at the beach when i felt a rumble from the depths of my stomach. i ignored it i didn't want my stomach to ruin our fun. i pushed down the feeling and continued. about 15min later the feeling was back and stronger than before. again i ignored it and continued. 5min later it felt like a nuclear reactor had just exploded in my stomach. i started running. i yelled to my friend to hurry the fuck up. \\n\\nrunning in sand is extremely hard if you did not know this. we got in his car and i yelled at him to floor it. my stomach was screaming and if he didn't hurry i was gonna have this baby in his car and it wasn't gonna be pretty. after a few red lights and me screaming like a woman in labor we made it to the store. \\n\\ni practically tore his car door open and ran inside. i ran to the bathroom opened the door and barely got my pants down before the dam burst and a flood of shit poured from my ass. \\n\\ni finished up when i felt something wet on my ass. i rubbed it thinking it was back splash. no, mass was covered in the after math of me abusing the toilet. i grabbed all the paper towels i could and gave my self a whores bath right there. \\n\\ni sprayed the bathroom down with the air freshener and left. an elderly lady walked in quickly and closed the door. i was just about to walk away when i heard gag. instead of walking i ran. i got to the car and told him to get the hell out of there.\"\n",
      "b'liking seafood'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 08:26:53.794760: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for document, title in reddit_ds:\n",
    "    print(document.numpy())\n",
    "    print(title.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    reddit_ds.map(lambda document, _: document)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "train_ds = train_ds.take(NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryCallback(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_batches,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.target_batches = target_batches\n",
    "        self.print_stats = print_stats\n",
    "\n",
    "        self.memory_usage = []\n",
    "        self.labels = []\n",
    "\n",
    "    def _compute_memory_usage(self):\n",
    "        memory_stats = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "        # Convert bytes to GB and store in list.\n",
    "        peak_usage = round(memory_stats[\"peak\"] / (2**30), 3)\n",
    "        self.memory_usage.append(peak_usage)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} start\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if batch in self.target_batches:\n",
    "            self._compute_memory_usage()\n",
    "            self.labels.append(f\"batch {batch}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text, max_length=200):\n",
    "    start = time.time()\n",
    "\n",
    "    output = model.generate(input_text, max_length=max_length)\n",
    "    print(\"\\nOutput:\")\n",
    "    print(output)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Total Time Elapsed: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_loss():\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        epsilon=1e-6,\n",
    "        global_clipnorm=1.0,  # Gradient clipping.\n",
    "    )\n",
    "    # Exclude layernorm and bias terms from weight decay.\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
    "\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
      "1042301/1042301 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
      "456318/456318 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
      "497986112/497986112 [==============================] - 11s 0us/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor: \"gpt2_causal_lm_preprocessor\"\n",
      "__________________________________________________________________________________________________\n",
      " Tokenizer (type)                                    Vocab #     \n",
      "==================================================================================================\n",
      " gpt2_tokenizer (GPT2Tokenizer)                      50257       \n",
      "__________________________________________________________________________________________________\n",
      "                                                                                                  \n",
      "Model: \"gpt2_causal_lm\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gpt2_backbone (GPT2Backbone)   (None, None, 768)    124439808   ['padding_mask[0][0]',           \n",
      "                                                                  'token_ids[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLambda)  (None, None, 50257)  0           ['gpt2_backbone[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "gpt2_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_memory_callback = GPUMemoryCallback(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "optimizer, loss = get_optimizer_and_loss()\n",
    "\n",
    "gpt2_lm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 08:24:21.764191: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2023-07-05 08:24:21.764722: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_34' with dtype int64 and shape [8]\n",
      "\t [[{{node Placeholder/_34}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching devices found for 'GPU:0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt2_lm\u001b[39m.\u001b[39;49mfit(train_ds, epochs\u001b[39m=\u001b[39;49mEPOCHS, callbacks\u001b[39m=\u001b[39;49m[gpu_memory_callback])\n\u001b[1;32m      2\u001b[0m gpt2_lm_memory_usage \u001b[39m=\u001b[39m gpu_memory_callback\u001b[39m.\u001b[39mmemory_usage\n",
      "File \u001b[0;32m~/miniconda/envs/qlora/lib/python3.9/site-packages/keras_nlp/src/utils/pipeline_model.py:191\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         (vx, vy, vsw) \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    185\u001b[0m             validation_data\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m         validation_data \u001b[39m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    188\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    189\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    192\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    193\u001b[0m     y\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    194\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    195\u001b[0m     sample_weight\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    196\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    197\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/qlora/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mGPUMemoryCallback.on_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_begin\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_memory_usage()\n\u001b[1;32m     23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m start\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mGPUMemoryCallback._compute_memory_usage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_memory_usage\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     memory_stats \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mget_memory_info(\u001b[39m\"\u001b[39;49m\u001b[39mGPU:0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Convert bytes to GB and store in list.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     peak_usage \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(memory_stats[\u001b[39m\"\u001b[39m\u001b[39mpeak\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m), \u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No matching devices found for 'GPU:0'"
     ]
    }
   ],
   "source": [
    "gpt2_lm.fit(train_ds, epochs=EPOCHS, callbacks=[gpu_memory_callback])\n",
    "gpt2_lm_memory_usage = gpu_memory_callback.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
